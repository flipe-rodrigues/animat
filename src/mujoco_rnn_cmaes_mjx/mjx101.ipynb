{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b474f2dc",
   "metadata": {},
   "source": [
    "# Brax: a differentiable physics engine\n",
    "\n",
    "[Brax](https://github.com/google/brax) simulates physical systems made up of rigid bodies, joints, and actutators.  Brax provides the function:\n",
    "\n",
    "$$\n",
    "\\text{state}_{t+1} = \\text{step}(\\text{system}, \\text{state}_t, \\text{act})\n",
    "$$\n",
    "\n",
    "where:\n",
    "* $\\text{system}$ is the static description of the physical system: each body in the world, its weight and size, and so on\n",
    "* $\\text{state}_t$ is the dynamic state of the system at time $t$: each body's position, rotation, velocity, and angular velocity\n",
    "* $\\text{act}$ is dynamic input to the system in the form of motor actuation\n",
    "\n",
    "Brax simulations are differentiable: the gradient $\\Delta \\text{step}$ can be used for efficient trajectory optimization.  But Brax is also well-suited to derivative-free optimization methods such as evolutionary strategy or reinforcement learning.\n",
    "\n",
    "Let's review how $\\text{system}$, $\\text{state}_t$, and $\\text{act}$ are used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372abb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import mediapy as media\n",
    "from etils import epath\n",
    "\n",
    "import mujoco\n",
    "import mujoco.mjx as mjx\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "\n",
    "from brax import envs\n",
    "from brax.envs.base import Env, PipelineEnv, State\n",
    "from brax.io import html, mjcf, model\n",
    "\n",
    "print(f\"jax.devices(): {jax.devices()}\")\n",
    "\n",
    "MJCF_ROOT_PATH = epath.Path(\"../../mujoco\")\n",
    "\n",
    "\n",
    "def zscore(x, xmean, xstd, default=0):\n",
    "    valid = jnp.greater(xstd, 0)\n",
    "    return jnp.where(valid, (x - xmean) / xstd, default)\n",
    "\n",
    "\n",
    "def l1_norm(x):\n",
    "    return jnp.sum(jnp.abs(x))\n",
    "\n",
    "\n",
    "def l2_norm(x):\n",
    "    return jnp.sqrt(jnp.sum(x**2))\n",
    "\n",
    "\n",
    "class SequentialReacher(PipelineEnv):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_duration=3,\n",
    "        num_targets=10,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.mj_model = mujoco.MjModel.from_xml_path(\n",
    "            (MJCF_ROOT_PATH / \"arm.xml\").as_posix()\n",
    "        )\n",
    "\n",
    "        sys = mjcf.load_model(self.mj_model)\n",
    "        kwargs[\"backend\"] = \"mjx\"\n",
    "\n",
    "        super().__init__(sys, **kwargs)\n",
    "\n",
    "        # Get the site ID using the name of your end effector\n",
    "        self.hand_id = self.mj_model.geom(\"hand\").id\n",
    "        self.target_id = self.mj_model.body_mocapid[\n",
    "            mujoco.mj_name2id(self.mj_model, mujoco.mjtObj.mjOBJ_BODY, b\"target\")\n",
    "        ]\n",
    "\n",
    "        # Load sensor stats\n",
    "        sensor_stats_path = os.path.join(MJCF_ROOT_PATH, \"sensor_stats.pkl\")\n",
    "        with open(sensor_stats_path, \"rb\") as f:\n",
    "            self.sensor_stats = pickle.load(f)\n",
    "\n",
    "        # Load hand stats\n",
    "        hand_position_stats_path = os.path.join(\n",
    "            MJCF_ROOT_PATH, \"hand_position_stats.pkl\"\n",
    "        )\n",
    "        with open(hand_position_stats_path, \"rb\") as f:\n",
    "            self.hand_position_stats = pickle.load(f)\n",
    "\n",
    "        # Load candidate target positions\n",
    "        candidate_targets_path = os.path.join(MJCF_ROOT_PATH, \"candidate_targets.pkl\")\n",
    "        with open(candidate_targets_path, \"rb\") as f:\n",
    "            self.candidate_targets = pickle.load(f)\n",
    "\n",
    "        # Load candidate nail positions\n",
    "        grid_positions_path = os.path.join(MJCF_ROOT_PATH, \"grid_positions.pkl\")\n",
    "        with open(grid_positions_path, \"rb\") as f:\n",
    "            self.grid_positions = pickle.load(f)\n",
    "\n",
    "        # Convert stats to JAX arrays\n",
    "        self.target_means = jnp.array(self.hand_position_stats[\"mean\"].values)\n",
    "        self.target_stds = jnp.array(self.hand_position_stats[\"std\"].values)\n",
    "        self.sensor_means = jnp.array(self.sensor_stats[\"mean\"].values)\n",
    "        self.sensor_stds = jnp.array(self.sensor_stats[\"std\"].values)\n",
    "\n",
    "        # Convert candidate_targets to JAX array\n",
    "        self.candidate_target_positions = jnp.array(self.candidate_targets.values)\n",
    "\n",
    "        self.target_duration = target_duration\n",
    "        self.num_targets = num_targets\n",
    "\n",
    "    def reset(self, key: jnp.ndarray) -> State:\n",
    "        \"\"\"Resets the environment to an initial state.\"\"\"\n",
    "        qpos = jnp.zeros(self.sys.nq)\n",
    "        qvel = jnp.zeros(self.sys.nv)\n",
    "        data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "        target_positions = self._sample_target_positions(key)\n",
    "        data = self._update_target(data, target_positions)\n",
    "\n",
    "        obs = self._get_obs(data)\n",
    "        reward, done = jnp.zeros(2)\n",
    "\n",
    "        return State(\n",
    "            data, obs, reward, done, info={\"target_positions\": target_positions}\n",
    "        )\n",
    "\n",
    "    def step(self, state: State, action: jnp.ndarray) -> State:\n",
    "        \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
    "        data = self.pipeline_step(state.pipeline_state, action)\n",
    "        data = self._update_target(data, state.info[\"target_positions\"])\n",
    "\n",
    "        hand_position = self._get_hand_pos(data)\n",
    "        target_position = self._get_target_pos(data)\n",
    "        euclidean_distance = l2_norm(target_position - hand_position)\n",
    "\n",
    "        obs = self._get_obs(data)\n",
    "        reward = -euclidean_distance\n",
    "\n",
    "        done = jnp.where(data.time > self.target_duration * self.num_targets, 1.0, 0.0)\n",
    "\n",
    "        return state.replace(pipeline_state=data, obs=obs, reward=reward, done=done)\n",
    "\n",
    "    def _get_obs(self, data: mjx.Data) -> jnp.ndarray:\n",
    "        target_position = self._get_target_pos(data)\n",
    "        sensor_data = data.sensordata.copy()\n",
    "        norm_target_position = zscore(\n",
    "            target_position,\n",
    "            self.target_means,\n",
    "            self.target_stds,\n",
    "        )\n",
    "        norm_sensor_data = zscore(\n",
    "            sensor_data,\n",
    "            self.sensor_means,\n",
    "            self.sensor_stds,\n",
    "        )\n",
    "        obs = jnp.concatenate(\n",
    "            [\n",
    "                norm_target_position,\n",
    "                norm_sensor_data,\n",
    "            ]\n",
    "        )\n",
    "        return jnp.reshape(obs, (1, -1))\n",
    "\n",
    "    def _sample_target_positions(self, key: jnp.ndarray):\n",
    "        \"\"\"Sample target positions (w/o replacement) from the candidate targets\"\"\"\n",
    "        sample_idcs = jax.random.choice(\n",
    "            key,\n",
    "            self.candidate_target_positions.shape[0],\n",
    "            shape=(self.num_targets,),\n",
    "            replace=False,\n",
    "        )\n",
    "        return self.candidate_target_positions[sample_idcs]\n",
    "\n",
    "    def _update_target(self, data: mjx.Data, target_positions) -> jnp.ndarray:\n",
    "        \"\"\"Update the target position\"\"\"\n",
    "        target_idx = jnp.floor_divide(data.time, self.target_duration).astype(jnp.int32)\n",
    "        mocap_position = data.mocap_pos.at[self.target_id].set(\n",
    "            target_positions[target_idx]\n",
    "        )\n",
    "        return data.replace(mocap_pos=mocap_position)\n",
    "\n",
    "    def _get_hand_pos(self, data: mjx.Data) -> jnp.ndarray:\n",
    "        \"\"\"Get the position of the end effector (hand)\"\"\"\n",
    "        hand_position = data.geom_xpos[self.hand_id].copy()\n",
    "        return hand_position\n",
    "\n",
    "    def _get_target_pos(self, data: mjx.Data) -> jnp.ndarray:\n",
    "        \"\"\"Get the position of the target\"\"\"\n",
    "        target_position = data.mocap_pos[self.target_id].copy()\n",
    "        return target_position\n",
    "\n",
    "\n",
    "envs.register_environment(\"sequential_reacher\", SequentialReacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e15b11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames: 15000\n",
      "Number of targets: 10\n",
      "Target duration: 3\n",
      "Time step: 0.0020000000949949026\n"
     ]
    }
   ],
   "source": [
    "# instantiate the environment\n",
    "env_name = \"sequential_reacher\"\n",
    "env = envs.get_environment(env_name, target_duration=3, num_targets=10)\n",
    "\n",
    "# define the jit reset/step functions\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "\n",
    "# initialize the state\n",
    "seed = 42\n",
    "state = jit_reset(jax.random.PRNGKey(seed))\n",
    "rollout = [state.pipeline_state]\n",
    "num_frames = jnp.int32(jnp.round(env.target_duration * env.num_targets / env.dt))\n",
    "\n",
    "print(f\"Number of frames: {num_frames}\")\n",
    "print(f\"Number of targets: {env.num_targets}\")\n",
    "print(f\"Target duration: {env.target_duration}\")\n",
    "print(f\"Time step: {env.dt}\")\n",
    "\n",
    "# grab a trajectory\n",
    "for i in range(num_frames):\n",
    "    prev_state = state.pipeline_state\n",
    "    ctrl = jnp.array([0.0, 0.01, 0.0, 0.1])\n",
    "    state = jit_step(state, ctrl)\n",
    "    rollout.append(state.pipeline_state)\n",
    "\n",
    "media.show_video(env.render(rollout), fps=1.0 / env.dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee76f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.es import train as es\n",
    "\n",
    "train_fn = functools.partial(\n",
    "    ppo.train,\n",
    "    num_timesteps=20_000_000,\n",
    "    num_evals=10,\n",
    "    reward_scaling=0.1,\n",
    "    episode_length=1000,\n",
    "    normalize_observations=True,\n",
    "    action_repeat=1,\n",
    "    unroll_length=10,\n",
    "    num_minibatches=24,\n",
    "    num_updates_per_batch=8,\n",
    "    discounting=0.97,\n",
    "    learning_rate=3e-4,\n",
    "    entropy_cost=1e-3,\n",
    "    num_envs=3072,\n",
    "    batch_size=512,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "# train_fn = functools.partial(\n",
    "#     es.train,\n",
    "#     population_size=128,\n",
    "#     num_eval_envs=128,\n",
    "#     num_timesteps=15_000,\n",
    "#     episode_length=15_000,\n",
    "#     max_devices_per_host=8,\n",
    "#     seed=1,\n",
    "# )\n",
    "\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "ydataerr = []\n",
    "times = [datetime.now()]\n",
    "\n",
    "max_y, min_y = 0, -1000\n",
    "\n",
    "\n",
    "def progress(num_steps, metrics):\n",
    "    times.append(datetime.now())\n",
    "    x_data.append(num_steps)\n",
    "    y_data.append(metrics[\"eval/episode_reward\"])\n",
    "    ydataerr.append(metrics[\"eval/episode_reward_std\"])\n",
    "\n",
    "    plt.xlim([-1, train_fn.keywords[\"num_timesteps\"] * 1.25])\n",
    "    plt.ylim([min_y, max_y])\n",
    "\n",
    "    plt.xlabel(\"# environment steps\")\n",
    "    plt.ylabel(\"reward per episode\")\n",
    "    plt.title(f\"y={y_data[-1]:.3f}\")\n",
    "\n",
    "    plt.errorbar(x_data, y_data, yerr=ydataerr)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "make_inference_fn, params, _ = train_fn(environment=env, progress_fn=progress)\n",
    "\n",
    "print(f\"time to jit: {times[1] - times[0]}\")\n",
    "print(f\"time to train: {times[-1] - times[1]}\")\n",
    "\n",
    "model_path = 'mjx_brax_policy'\n",
    "model.save_params(model_path, params)\n",
    "\n",
    "params = model.load_params(model_path)\n",
    "\n",
    "inference_fn = make_inference_fn(params)\n",
    "jit_inference_fn = jax.jit(inference_fn)\n",
    "\n",
    "eval_env = envs.get_environment(env_name)\n",
    "\n",
    "jit_reset = jax.jit(eval_env.reset)\n",
    "jit_step = jax.jit(eval_env.step)\n",
    "\n",
    "# initialize the state\n",
    "key = jax.random.PRNGKey(0)\n",
    "state = jit_reset(key)\n",
    "rollout = [state.pipeline_state]\n",
    "\n",
    "# grab a trajectory\n",
    "n_steps = 500\n",
    "render_every = 1\n",
    "\n",
    "for i in range(n_steps):\n",
    "  act_rng, key = jax.random.split(key)\n",
    "  ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "  state = jit_step(state, ctrl)\n",
    "  rollout.append(state.pipeline_state)\n",
    "\n",
    "  if state.done:\n",
    "    break\n",
    "\n",
    "media.show_video(env.render(rollout[::render_every]), fps=1.0 / env.dt / render_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Tuple\n",
    "from flax import linen as nn\n",
    "from flax.struct import dataclass\n",
    "from typing import Callable, Any, Optional\n",
    "\n",
    "from evojax.policy.base import PolicyState\n",
    "from evojax.policy.base import PolicyNetwork\n",
    "from evojax.task.base import TaskState\n",
    "from evojax.util import get_params_format_fn\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class State(PolicyState):\n",
    "    hx: jnp.ndarray\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \"\"\"A recurrent neural network.\"\"\"\n",
    "\n",
    "    input_dim: int\n",
    "    hidden_dim: int\n",
    "    output_dim: int\n",
    "    hidden_act_fn: Callable = nn.tanh\n",
    "    output_act_fn: Callable = nn.sigmoid\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, hx):\n",
    "        new_hx = nn.SimpleCell()(hx, x)\n",
    "        y = nn.tanh(new_hx)\n",
    "        return y, new_hx  # In addition to the output, we return hx too.\n",
    "\n",
    "\n",
    "class RNNPolicy(PolicyNetwork):\n",
    "    \"\"\"The policy wraps the model and does some data formatting works.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "\n",
    "        model = RNN(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=output_dim,\n",
    "            hidden_act_fn=nn.tanh,\n",
    "            output_act_fn=nn.sigmoid,\n",
    "        )\n",
    "        params = model.init(\n",
    "            jax.random.PRNGKey(0),\n",
    "            jnp.ones([1, input_dim]),\n",
    "            jnp.zeros([1, self.hidden_dim]),\n",
    "        )\n",
    "        self.num_params, format_params_fn = get_params_format_fn(params)\n",
    "        print(\"RNNPolicy.num_params = {}\".format(self.num_params))\n",
    "\n",
    "        self._format_params_fn = jax.vmap(format_params_fn)\n",
    "        self._forward_fn = jax.vmap(model.apply)\n",
    "\n",
    "    def reset(self, states: TaskState) -> PolicyState:\n",
    "        keys = jax.random.split(jax.random.PRNGKey(0), states.obs.shape[0])\n",
    "        b_size, obs_dim = states.obs.shape\n",
    "        lstm_h = (\n",
    "            jnp.zeros([b_size, self.hidden_dim]),\n",
    "            jnp.zeros([b_size, self.hidden_dim]),\n",
    "        )\n",
    "        return State(keys=keys, hx=lstm_h)\n",
    "\n",
    "    def get_actions(\n",
    "        self, t_states: TaskState, params: jnp.ndarray, p_states: State\n",
    "    ) -> Tuple[jnp.ndarray, State]:\n",
    "        # Calling `self._format_params_fn` unflattens the parameters so that\n",
    "        # our Flax model can take that as an input.\n",
    "        params = self._format_params_fn(params)\n",
    "\n",
    "        # Now we return the actions and the updated `p_states`.\n",
    "        actions, hx = self._forward_fn(params, t_states.obs, p_states.hx)\n",
    "        return actions, State(keys=p_states.keys, hx=hx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c852c326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Import Libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from brax import envs\n",
    "from brax.io import html\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "from evojax import SimManager\n",
    "from evojax import ObsNormalizer\n",
    "from evojax.algo import PGPE\n",
    "from evojax.algo import CMA_ES\n",
    "from evojax.policy import MLPPolicy\n",
    "from evojax.task.brax_task import BraxTask\n",
    "\n",
    "import os\n",
    "\n",
    "# @title Preview a Brax environment { run: \"auto\" }\n",
    "# @markdown Select the environment to train:\n",
    "env_name = \"sequential_reacher\"  # @param ['ant', 'humanoid', 'halfcheetah', 'fetch']\n",
    "env = envs.create(env_name=env_name)\n",
    "# env = env_fn()\n",
    "state = env.reset(rng=jax.random.PRNGKey(seed=0))\n",
    "\n",
    "HTML(html.render(env.sys, [state.pipeline_state]))\n",
    "\n",
    "# @title Set hyper-parameters\n",
    "# @markdown PLEASE NOTE: `pop_size` and `num_tests` should be multiples of `jax.local_device_count()`.\n",
    "\n",
    "n_devices = jax.local_device_count()\n",
    "\n",
    "pop_size = 1024  # @param\n",
    "num_tests = 128  # @param\n",
    "assert pop_size % n_devices == 0\n",
    "assert num_tests % n_devices == 0\n",
    "\n",
    "max_iters = 300  # @param\n",
    "center_lr = 0.01  # @param\n",
    "init_std = 0.04  # @param\n",
    "std_lr = 0.07  # @param\n",
    "\n",
    "seed = 42  # @param\n",
    "\n",
    "# @title Training\n",
    "train_task = BraxTask(env_name=env_name, test=False)\n",
    "test_task = BraxTask(env_name=env_name, test=True)\n",
    "\n",
    "policy = MLPPolicy(\n",
    "    input_dim=train_task.obs_shape[0],\n",
    "    output_dim=train_task.act_shape[0],\n",
    "    hidden_dims=[32, 32, 32, 32],\n",
    ")\n",
    "print(\"#params={}\".format(policy.num_params))\n",
    "\n",
    "solver = PGPE(\n",
    "    pop_size=pop_size,\n",
    "    param_size=policy.num_params,\n",
    "    optimizer=\"adam\",\n",
    "    center_learning_rate=center_lr,\n",
    "    stdev_learning_rate=std_lr,\n",
    "    init_stdev=init_std,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "# solver = CMA_ES(\n",
    "#     pop_size=pop_size,\n",
    "#     param_size=policy.num_params,\n",
    "#     init_stdev=1.0,\n",
    "#     seed=seed,\n",
    "# )\n",
    "\n",
    "obs_normalizer = ObsNormalizer(obs_shape=train_task.obs_shape)\n",
    "\n",
    "sim_mgr = SimManager(\n",
    "    n_repeats=1,\n",
    "    test_n_repeats=1,\n",
    "    pop_size=pop_size,\n",
    "    n_evaluations=num_tests,\n",
    "    policy_net=policy,\n",
    "    train_vec_task=train_task,\n",
    "    valid_vec_task=test_task,\n",
    "    seed=seed,\n",
    "    obs_normalizer=obs_normalizer,\n",
    ")\n",
    "\n",
    "print(\"Start training Brax ({}) for {} iterations.\".format(env_name, max_iters))\n",
    "start_time = time.perf_counter()\n",
    "for train_iters in range(max_iters):\n",
    "\n",
    "    # Training\n",
    "    params = solver.ask()\n",
    "    scores, _ = sim_mgr.eval_params(params=params, test=False)\n",
    "    solver.tell(fitness=scores)\n",
    "\n",
    "    # Test periodically.\n",
    "    if train_iters > 0 and train_iters % 10 == 0:\n",
    "        best_params = solver.best_params\n",
    "        scores = np.array(sim_mgr.eval_params(params=best_params, test=True)[0])\n",
    "        score_avg = np.mean(scores)\n",
    "        score_std = np.std(scores)\n",
    "        print(\n",
    "            \"Iter={0}, #tests={1}, score.avg={2:.2f}, score.std={3:.2f}\".format(\n",
    "                train_iters, num_tests, score_avg, score_std\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Final test.\n",
    "best_params = solver.best_params\n",
    "scores = np.array(sim_mgr.eval_params(params=best_params, test=True)[0])\n",
    "score_avg = np.mean(scores)\n",
    "score_std = np.std(scores)\n",
    "print(\n",
    "    \"Iter={0}, #tests={1}, score.avg={2:.2f}, score.std={3:.2f}\".format(\n",
    "        train_iters, num_tests, score_avg, score_std\n",
    "    )\n",
    ")\n",
    "print(\"time cost: {}s\".format(time.perf_counter() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualize the trained policy\n",
    "env = envs.create(env_name=\"sequential_reacher\", num_targets=10, target_duration=3)\n",
    "\n",
    "task_reset_fn = jax.jit(env.reset)\n",
    "policy_reset_fn = jax.jit(policy.reset)\n",
    "step_fn = jax.jit(env.step)\n",
    "act_fn = jax.jit(policy.get_actions)\n",
    "obs_norm_fn = jax.jit(obs_normalizer.normalize_obs)\n",
    "\n",
    "best_params = solver.best_params\n",
    "obs_params = sim_mgr.obs_params\n",
    "\n",
    "total_reward = 0\n",
    "rng = jax.random.PRNGKey(seed=42)\n",
    "task_state = task_reset_fn(rng=rng)\n",
    "policy_state = policy_reset_fn(task_state)\n",
    "\n",
    "rollout = [task_state.pipeline_state]\n",
    "\n",
    "while not task_state.done:\n",
    "    task_state = task_state.replace(obs=task_state.obs)\n",
    "    act, policy_state = act_fn(task_state, best_params[None, :], policy_state)\n",
    "    task_state = step_fn(task_state, act[0])\n",
    "    total_reward = total_reward + task_state.reward\n",
    "    rollout.append(task_state.pipeline_state)\n",
    "\n",
    "print(\"rollout reward = {}\".format(total_reward))\n",
    "\n",
    "media.show_video(env.render(rollout), fps=1.0 / env.dt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
